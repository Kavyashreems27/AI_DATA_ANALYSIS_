{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1azhh6D8JdIS6lAU3feMmwaDIJ0fU9nRi","authorship_tag":"ABX9TyPDYmL8uhLA6foqC7CYWYut"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Advanced Data Cleaning with Multiple Issues\n","\n","import pandas as pd\n","import numpy as np\n","import re\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Step 1: Load the dataset\n","def load_data(filepath):\n","    try:\n","        df = pd.read_csv(filepath)\n","    except Exception as e:\n","        print(f\"Error loading data: {e}\")\n","        return None\n","    return df\n","\n","# Step 2: Standardize column names\n","def standardize_column_names(df):\n","    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '').str.replace('[^a-z0-9]', '', regex=True)\n","    return df\n","\n","# Step 3: Remove duplicate rows\n","def remove_duplicates(df):\n","    initial_shape = df.shape\n","    df = df.drop_duplicates()\n","    final_shape = df.shape\n","    print(f\"Removed {initial_shape[0] - final_shape[0]} duplicate rows.\")\n","    return df\n","\n","# Step 4: Handle missing values\n","def handle_missing_values(df):\n","    for column in df.columns:\n","        if df[column].dtype in ['float64', 'int64']:\n","            imputer = SimpleImputer(strategy='median')\n","            df[column] = imputer.fit_transform(df[[column]])\n","        else:\n","            df[column] = df[column].fillna(df[column].mode()[0])\n","    return df\n","\n","# Step 5: Standardize text data\n","def standardize_text_data(df):\n","    for column in df.select_dtypes(include='object').columns:\n","        df[column] = df[column].str.strip().str.lower()\n","    return df\n","\n","# Step 6: Handle inconsistent categorical entries\n","def fix_categorical_inconsistencies(df, column, mapping_dict):\n","    df[column] = df[column].replace(mapping_dict)\n","    return df\n","\n","# Step 7: Detect and handle outliers using IQR\n","def handle_outliers(df):\n","    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n","    for col in numeric_cols:\n","        Q1 = df[col].quantile(0.25)\n","        Q3 = df[col].quantile(0.75)\n","        IQR = Q3 - Q1\n","        lower_bound = Q1 - 1.5 * IQR\n","        upper_bound = Q3 + 1.5 * IQR\n","        df[col] = np.where(df[col] < lower_bound, lower_bound,\n","                           np.where(df[col] > upper_bound, upper_bound, df[col]))\n","    return df\n","\n","# Step 8: Encode categorical variables\n","def encode_categorical(df):\n","    label_encoders = {}\n","    for column in df.select_dtypes(include='object').columns:\n","        le = LabelEncoder()\n","        df[column] = le.fit_transform(df[column])\n","        label_encoders[column] = le\n","    return df, label_encoders\n","\n","# Step 9: Final validation\n","def validate_data(df):\n","    print(\"Data types:\\n\", df.dtypes)\n","    print(\"\\nMissing values:\\n\", df.isnull().sum())\n","    print(\"\\nData preview:\\n\", df.head())\n","    return\n","\n","# Main execution\n","if _name_ == \"_main_\":\n","    # Replace 'your_dataset.csv' with your actual dataset path\n","    filepath = 'your_dataset.csv'\n","    df = load_data(filepath)\n","    if df is not None:\n","        df = standardize_column_names(df)\n","        df = remove_duplicates(df)\n","        df = handle_missing_values(df)\n","        df = standardize_text_data(df)\n","        # Example: Fix inconsistencies in 'gender' column\n","        gender_mapping = {'m': 'male', 'f': 'female', 'male': 'male', 'female': 'female'}\n","        if 'gender' in df.columns:\n","            df = fix_categorical_inconsistencies(df, 'gender', gender_mapping)\n","        df = handle_outliers(df)\n","        df, encoders = encode_categorical(df)\n","        validate_data(df)\n","        # Optionally, save the cleaned data\n","        df.to_csv('cleaned_data.csv', index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"wgBIKD0YQ88Q","executionInfo":{"status":"error","timestamp":1746110013838,"user_tz":-330,"elapsed":1517,"user":{"displayName":"Kavyashree MS","userId":"02983501451928927214"}},"outputId":"9bf182ea-1b5e-457d-b5fc-9a317a15625f"},"execution_count":4,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name '_name_' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-74a2577ceda1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# Main execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0m_name_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"_main_\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;31m# Replace 'your_dataset.csv' with your actual dataset path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'your_dataset.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name '_name_' is not defined"]}]}]}