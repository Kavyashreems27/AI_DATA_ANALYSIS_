{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1azhh6D8JdIS6lAU3feMmwaDIJ0fU9nRi","authorship_tag":"ABX9TyN0luDsN2srh2gMBFBw4htC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Hands-on Data Cleaning & Quality Automation - Ques 1\n","\n","import pandas as pd\n","import numpy as np\n","from sklearn.impute import SimpleImputer\n","import os\n","\n","# Step 1: Load the dataset\n","def load_data(filepath):\n","    _, file_ext = os.path.splitext(filepath)\n","    if file_ext == '.csv':\n","        df = pd.read_csv(filepath)\n","    elif file_ext == '.xlsx':\n","        df = pd.read_excel(filepath)\n","    else:\n","        raise ValueError(\"Unsupported file format\")\n","    return df\n","\n","# Step 2: Assess data quality\n","def check_data_quality(df):\n","    quality_report = {\n","        'missing_values': df.isnull().sum().to_dict(),\n","        'duplicates': df.duplicated().sum(),\n","        'total_rows': len(df),\n","        'memory_usage_MB': round(df.memory_usage().sum() / 1024**2, 2)\n","    }\n","    return quality_report\n","\n","# Step 3: Standardize data types\n","def standardize_datatypes(df):\n","    for column in df.columns:\n","        if df[column].dtype == 'object':\n","            try:\n","                df[column] = pd.to_datetime(df[column])\n","                print(f\"Converted {column} to datetime\")\n","            except (ValueError, TypeError):\n","                try:\n","                    df[column] = pd.to_numeric(df[column].str.replace('[^\\d.]', '', regex=True))\n","                    print(f\"Converted {column} to numeric\")\n","                except:\n","                    pass\n","    return df\n","\n","# Step 4: Handle missing values\n","def handle_missing_values(df):\n","    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n","    categorical_cols = df.select_dtypes(include=['object']).columns\n","\n","    if len(numeric_cols) > 0:\n","        num_imputer = SimpleImputer(strategy='median')\n","        df[numeric_cols] = num_imputer.fit_transform(df[numeric_cols])\n","\n","    if len(categorical_cols) > 0:\n","        cat_imputer = SimpleImputer(strategy='most_frequent')\n","        df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])\n","\n","    return df\n","\n","# Step 5: Detect and handle outliers\n","def remove_outliers(df):\n","    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n","    outliers_removed = {}\n","\n","    for col in numeric_cols:\n","        Q1 = df[col].quantile(0.25)\n","        Q3 = df[col].quantile(0.75)\n","        IQR = Q3 - Q1\n","        lower = Q1 - 1.5 * IQR\n","        upper = Q3 + 1.5 * IQR\n","        outliers = df[(df[col] < lower) | (df[col] > upper)].shape[0]\n","        df[col] = df[col].clip(lower=lower, upper=upper)\n","        if outliers > 0:\n","            outliers_removed[col] = outliers\n","\n","    return df, outliers_removed\n","\n","# Step 6: Validate the results\n","def validate_cleaning(df):\n","    report = {\n","        'missing_values_remaining': df.isnull().sum().sum(),\n","        'duplicates_remaining': df.duplicated().sum(),\n","        'data_types': df.dtypes.to_dict()\n","    }\n","    return report\n","\n","# Main execution\n","if _name_ == \"_main_\":\n","    # Replace 'your_dataset.csv' with your actual dataset path\n","    filepath = 'your_dataset.csv'\n","    df = load_data(filepath)\n","\n","    print(\"Initial Data Quality Report:\")\n","    print(check_data_quality(df))\n","\n","    df = standardize_datatypes(df)\n","    df = handle_missing_values(df)\n","    df, outliers_removed = remove_outliers(df)\n","\n","    print(\"\\nOutliers Removed:\")\n","    print(outliers_removed)\n","\n","    print(\"\\nFinal Validation Report:\")\n","    print(validate_cleaning(df))\n","\n","    # Optionally, save the cleaned data\n","    df.to_csv('cleaned_data.csv', index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"2oXUaa92KhKP","executionInfo":{"status":"error","timestamp":1746108346962,"user_tz":-330,"elapsed":1672,"user":{"displayName":"Kavyashree MS","userId":"02983501451928927214"}},"outputId":"e38552b1-5967-451d-f236-12bb9d2df482"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name '_name_' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-b6ff80e763b5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m# Main execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0m_name_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"_main_\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;31m# Replace 'your_dataset.csv' with your actual dataset path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'your_dataset.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name '_name_' is not defined"]}]}]}