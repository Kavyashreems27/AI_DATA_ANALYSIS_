{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1azhh6D8JdIS6lAU3feMmwaDIJ0fU9nRi","authorship_tag":"ABX9TyMd84b3G6c1p6WIbd+GTEds"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.impute import SimpleImputer\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","\n","def preprocess_data(df, target_column=None):\n","    \"\"\"\n","    Applies a comprehensive set of data preprocessing steps to the input DataFrame.\n","\n","    Args:\n","        df (pd.DataFrame): The input DataFrame to preprocess.\n","        target_column (str, optional): The name of the target variable column.\n","                                       If provided, it will be separated before preprocessing\n","                                       and rejoined at the end. Defaults to None.\n","\n","    Returns:\n","        tuple: A tuple containing the preprocessed features (pd.DataFrame) and\n","               the target variable (pd.Series) if target_column is provided,\n","               otherwise just the preprocessed features (pd.DataFrame).\n","    \"\"\"\n","    X = df.copy()\n","    y = None\n","    if target_column in X.columns:\n","        y = X[target_column]\n","        X = X.drop(columns=[target_column])\n","\n","    # Identify numerical and categorical columns\n","    numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n","    categorical_cols = X.select_dtypes(include='object').columns.tolist()\n","\n","    # Create preprocessing pipelines for numerical and categorical features\n","    numerical_transformer = Pipeline(steps=[\n","        ('imputer', SimpleImputer(strategy='median')),  # Handle missing values with median\n","        ('scaler', StandardScaler())                    # Scale numerical features\n","    ])\n","\n","    categorical_transformer = Pipeline(steps=[\n","        ('imputer', SimpleImputer(strategy='most_frequent')), # Handle missing values with most frequent\n","        ('onehot', OneHotEncoder(handle_unknown='ignore'))    # One-hot encode categorical features\n","    ])\n","\n","    # Create a preprocessor using ColumnTransformer\n","    preprocessor = ColumnTransformer(\n","        transformers=[\n","            ('num', numerical_transformer, numerical_cols),\n","            ('cat', categorical_transformer, categorical_cols)])\n","\n","    # Fit and transform the data\n","    X_processed = preprocessor.fit_transform(X)\n","\n","    # Get feature names after one-hot encoding\n","    feature_names = preprocessor.get_feature_names_out(input_features=X.columns)\n","    X_processed_df = pd.DataFrame(X_processed, columns=feature_names, index=X.index)\n","\n","    if y is not None:\n","        return X_processed_df, y\n","    else:\n","        return X_processed_df\n","\n","def handle_outliers_iqr(df, columns, threshold=1.5):\n","    \"\"\"\n","    Handles outliers in specified columns of a DataFrame using the IQR method.\n","\n","    Args:\n","        df (pd.DataFrame): The input DataFrame.\n","        columns (list): A list of column names to handle outliers in.\n","        threshold (float): The multiplier for the IQR to define outlier boundaries.\n","\n","    Returns:\n","        pd.DataFrame: The DataFrame with outliers capped at the IQR boundaries.\n","    \"\"\"\n","    df_cleaned = df.copy()\n","    for col in columns:\n","        if col in df_cleaned.columns and pd.api.types.is_numeric_dtype(df_cleaned[col]):\n","            Q1 = df_cleaned[col].quantile(0.25)\n","            Q3 = df_cleaned[col].quantile(0.75)\n","            IQR = Q3 - Q1\n","            lower_bound = Q1 - threshold * IQR\n","            upper_bound = Q3 + threshold * IQR\n","            df_cleaned[col] = np.where(df_cleaned[col] < lower_bound, lower_bound, df_cleaned[col])\n","            df_cleaned[col] = np.where(df_cleaned[col] > upper_bound, upper_bound, df_cleaned[col])\n","    return df_cleaned\n","\n","def remove_duplicate_rows(df):\n","    \"\"\"\n","    Removes duplicate rows from a DataFrame.\n","\n","    Args:\n","        df (pd.DataFrame): The input DataFrame.\n","\n","    Returns:\n","        pd.DataFrame: The DataFrame with duplicate rows removed.\n","    \"\"\"\n","    df_no_duplicates = df.drop_duplicates().reset_index(drop=True)\n","    return df_no_duplicates\n","\n","def handle_imbalanced_data(X, y, method='oversampling', random_state=None):\n","    \"\"\"\n","    Handles imbalanced datasets using either oversampling or undersampling.\n","\n","    Args:\n","        X (pd.DataFrame): The feature matrix.\n","        y (pd.Series): The target variable.\n","        method (str): 'oversampling' or 'undersampling'. Defaults to 'oversampling'.\n","        random_state (int, optional): Random seed for reproducibility. Defaults to None.\n","\n","    Returns:\n","        tuple: The resampled feature matrix (pd.DataFrame) and target variable (pd.Series).\n","               Returns the original data if the method is not recognized.\n","    \"\"\"\n","    if method == 'oversampling':\n","        from imblearn.over_sampling import SMOTE\n","        smote = SMOTE(random_state=random_state)\n","        X_resampled, y_resampled = smote.fit_resample(X, y)\n","        return pd.DataFrame(X_resampled, columns=X.columns), pd.Series(y_resampled)\n","    elif method == 'undersampling':\n","        from imblearn.under_sampling import RandomUnderSampler\n","        rus = RandomUnderSampler(random_state=random_state)\n","        X_resampled, y_resampled = rus.fit_resample(X, y)\n","        return pd.DataFrame(X_resampled, columns=X.columns), pd.Series(y_resampled)\n","    else:\n","        print(f\"Warning: Method '{method}' not recognized. Returning original data.\")\n","        return X, y\n","\n","if _name_ == \"_main_\":\n","    # Sample DataFrame (replace with your actual data loading)\n","    data = {\n","        'numerical_col1': [1, 5, 2, np.nan, 7, 100, 3],\n","        'numerical_col2': [0.1, 0.5, 0.2, 0.8, np.nan, 0.3, 0.6],\n","        'categorical_col1': ['A', 'B', 'A', 'C', 'B', 'A', np.nan],\n","        'categorical_col2': ['X', 'Y', 'X', 'Z', 'Y', 'X', 'Y'],\n","        'target_variable': [0, 1, 0, 1, 0, 1, 0]\n","    }\n","    df = pd.DataFrame(data)\n","\n","    print(\"Original DataFrame:\")\n","    print(df)\n","    print(\"\\n\" + \"=\"*50 + \"\\n\")\n","\n","    # Separate target variable if it exists\n","    TARGET_COLUMN = 'target_variable'\n","    if TARGET_COLUMN in df.columns:\n","        X, y = df.drop(columns=[TARGET_COLUMN]), df[TARGET_COLUMN]\n","    else:\n","        X, y = df, None\n","\n","    # Handle missing values and scale/encode\n","    X_processed = preprocess_data(X)\n","    print(\"DataFrame after missing value handling, scaling, and encoding:\")\n","    print(X_processed)\n","    print(\"\\n\" + \"=\"*50 + \"\\n\")\n","\n","    # Handle outliers in numerical columns\n","    numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n","    X_processed_no_outliers = handle_outliers_iqr(X_processed, numerical_cols)\n","    print(\"DataFrame after outlier handling (IQR method):\")\n","    print(X_processed_no_outliers)\n","    print(\"\\n\" + \"=\"*50 + \"\\n\")\n","\n","    # Remove duplicate rows\n","    X_processed_unique = remove_duplicate_rows(X_processed_no_outliers)\n","    print(\"DataFrame after removing duplicate rows:\")\n","    print(X_processed_unique)\n","    print(\"\\n\" + \"=\"*50 + \"\\n\")\n","\n","    # Handle imbalanced data if a target variable exists\n","    if y is not None:\n","        X_balanced, y_balanced = handle_imbalanced_data(X_processed_unique, y, method='oversampling', random_state=42)\n","        print(\"DataFrame after handling imbalanced data (oversampling):\")\n","        print(\"Resampled Features:\")\n","        print(X_balanced)\n","        print(\"\\nResampled Target:\")\n","        print(y_balanced.value_counts())\n","        print(\"\\n\" + \"=\"*50 + \"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"XV6poCpC9v4y","executionInfo":{"status":"error","timestamp":1746373392980,"user_tz":-330,"elapsed":759,"user":{"displayName":"Kavyashree MS","userId":"02983501451928927214"}},"outputId":"dbe3b6e7-89b4-40e1-99d4-3022047b6d3b"},"execution_count":22,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name '_name_' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-dff7efe0851d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0m_name_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"_main_\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0;31m# Sample DataFrame (replace with your actual data loading)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     data = {\n","\u001b[0;31mNameError\u001b[0m: name '_name_' is not defined"]}]}]}